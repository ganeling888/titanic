---
title: "Titanic"
author: "GAN LING"
date: "2017/5/15"
output: html_document
---

install the neccessary package

```{r}
# install.packages("caret")
# install.packages("randomForest")
# install.packages("e1071")
# install.packages("psych")
# install.packages("lattice")
# install.packages("lsr")
# install.packages("rpart")
# install.packages("ada")
# install.packages("plyr")
# install.packages("kernlab")
# install.packages("LiblineaR")
# install.packages("klaR")
```

load all the neccessary package

```{r}
library(ggplot2)
library(psych)
library(lsr)
library(caret)
library(randomForest)
library(lattice)
library(e1071)
library(rpart)
library(ada)
library(plyr)
library(kernlab)
library(LiblineaR)
library(klaR)
library(MASS)
```

read the data

```{r}
trainSet <- read.csv("train.csv")
testSet <- read.csv("test.csv")
# convert some of them to factor
trainSet$Survived <- factor(trainSet$Survived)
trainSet$Pclass <- factor(trainSet$Pclass)
```

clean the data

```{r}
summary(trainSet)
# From our result only Age has NA values. We get that Age has 177 
# NA values, Cabin has 687 blank values, and Embarked has 2 blank values. 
# We do not use Cabin, so we do not deal with it. The 2 blank 
# values maybe affect our result. And most of Embarked are S 
# so we define two blank values to S
trainSet$Embarked[trainSet$Embarked==""] = "S"
```

see all of the variables we use in ggplot2

```{r}
# Pclass command
ggplot(trainSet, aes(x=Pclass,fill=Pclass)) +
    geom_bar() +
    xlab("Pclass") +
    ylab("Number of Each Class") +
    ggtitle("Pclass") +
    theme_bw()
# Sex command
ggplot(data=trainSet, aes(x=Sex, fill=Sex)) +
    geom_bar() +
    ggtitle("Sex") +
    labs(y="Number of Sex", x="Sex") +
    theme_bw()
# Age command
ggplot(trainSet, aes(x="", y=Age)) +
    stat_boxplot(geom ='errorbar')+
    geom_boxplot(fill="lightblue") +
    xlab("") +
    guides(fill=FALSE) +
    ggtitle("Age") +
    coord_flip() +
    theme_bw()
# SibSp command
ggplot(trainSet, aes(x=SibSp)) +
    geom_histogram() +
    geom_histogram(fill="lightblue") +
    xlab("SibSp") +
    ylab("Number") +
    guides(fill=FALSE) +
    ggtitle("SibSp") +
    theme_bw()
# Parch command
ggplot(trainSet, aes(x=Parch)) +
    geom_histogram() +
    geom_histogram(fill="lightblue") +
    xlab("Parch") +
    ylab("Number") +
    guides(fill=FALSE) +
    ggtitle("Parch") +
    theme_bw()
# Fare command
ggplot(trainSet, aes(x="", y=Fare)) +
    stat_boxplot(geom ='errorbar')+
    geom_boxplot(fill="lightblue") +
    xlab("") +
    guides(fill=FALSE) +
    ggtitle("Fare") +
    coord_flip() +
    theme_bw()
# Embarked command
ggplot(trainSet, aes(x=Embarked,fill=Embarked)) +
    geom_bar() +
    xlab("Embarked") +
    ylab("Number of Each Class") +
    ggtitle("Embarked") +
    theme_bw()
```

get some statistical test to see the correlation between single variable and output

```{r}
# Pclass
associationTest(~ Survived + Pclass, data = trainSet)
# Sex
associationTest(~ Survived + Sex, data = trainSet)
# Age
independentSamplesTTest(Age ~ Survived, data = trainSet)
# SibSp
independentSamplesTTest(SibSp ~ Survived, data = trainSet)
# Parch
independentSamplesTTest(Parch ~ Survived, data = trainSet)
# Fare
independentSamplesTTest(Fare ~ Survived, data = trainSet)
# Embarked
associationTest(~ Survived + Embarked, data = trainSet)


#draw them
# Pclass command
mosaicplot(table(trainSet$Pclass,trainSet$Survived),
           ylab="Survived",xlab="Pclass",
           color=c("lightblue","lightcoral"), 
           main="Survived by Pclass")
# Sex command
mosaicplot(table(trainSet$Sex,trainSet$Survived),
           ylab="Survived",xlab="Sex",
           color=c("lightblue","lightcoral"), 
           main="Survived by Sex")
# Age command
ggplot(trainSet, aes(x=Survived, y=Age, fill=Survived)) +
    stat_boxplot(geom ='errorbar') +
    geom_boxplot() +
    guides(fill=FALSE) +
    coord_flip() +
    ggtitle("Survived by Age") +
    theme_bw()
# SibSp command
ggplot(trainSet, aes(x=Survived, y=SibSp, fill=Survived)) +
    stat_boxplot(geom ='errorbar') +
    geom_boxplot() +
    guides(fill=FALSE) +
    coord_flip() +
    ggtitle("Survived by SibSp") +
    theme_bw()
# Parch command
ggplot(trainSet, aes(x=Survived, y=Parch, fill=Survived)) +
    stat_boxplot(geom ='errorbar') +
    geom_boxplot() +
    guides(fill=FALSE) +
    coord_flip() +
    ggtitle("Survived by Parch") +
    theme_bw()
# Fare command
ggplot(trainSet, aes(x=Survived, y=Fare, fill=Survived)) +
    stat_boxplot(geom ='errorbar') +
    geom_boxplot() +
    guides(fill=FALSE) +
    coord_flip() +
    ggtitle("Survived by Fare") +
    theme_bw()
# Embarked command
mosaicplot(table(trainSet$Embarked,trainSet$Survived),
           ylab="Survived",xlab="Embarked",
           color=c("lightblue","lightcoral"), 
           main="Survived by Embarked")
```

use glm model to do the train

```{r}
summary(glm(Survived ~ Pclass+Age+Sex+SibSp+Parch+Fare+Embarked, 
            data = trainSet, family = "binomial"))

```

do a backward step test for the training set using glm

```{r}
step(object=glm(Survived ~ Pclass+Age+Sex+SibSp+Parch+Fare+Embarked, 
                data = trainSet, family = "binomial"), 
                direction = "backward")
```

because there is so many extreme data in Fare, we can use log to normalize the data

```{r}
trainSet$logFare<-log(trainSet$Fare)
```

use new data to do the training

we dealed with Fare using log, and there are some Fare values equal to 0, log(0) is -Inf, we need to ignore all the -Inf before training

```{r}
step(object=glm(Survived ~ Pclass+Age+Sex+SibSp+Parch+logFare+Embarked, 
                data = trainSet[!is.infinite(trainSet$logFare),], 
                family = "binomial"), direction = "backward")
```

about the age, we use a random number between maximum and minimum as NA's age.

```{r}
set.seed(as.numeric(as.Date("2017-05-15")))
trainSet$totalAge <- ifelse(is.na(trainSet$Age), 
                            as.integer(runif(177,min=0.42,max=80)), 
                            trainSet$Age)					
```

train the data using Random Forest model

```{r}
set.seed(as.numeric(as.Date("2017-05-15")))
model <- train(Survived ~ Pclass + totalAge + Sex + SibSp + Embarked, 
               data = trainSet, 
               # Use the trainSet dataframe as the training data
               method = "rf",
               # Use the "random forest" algorithm
               trControl = trainControl(method = "cv", 
                                        # Use cross-validation
                                        number = 10) 
                                # Use 10 folds for cross-validation
)
model
# the accuracy rate is 0.8215438
```

now we can draw the graph

```{r}
extractFeatures <- function(data) {
    features <- c("Pclass",
                  "totalAge",
                  "Sex",
                  "SibSp",
                  "Embarked")
    fea <- data[,features]
    # fea$Embarked[fea$Embarked==""] = "S"
    fea$Sex      <- as.factor(fea$Sex)
    fea$Embarked <- as.factor(fea$Embarked)
    return(fea)
}
rf <- randomForest(extractFeatures(trainSet), as.factor(trainSet$Survived), ntree=100, importance=TRUE)
imp <- importance(rf, type=1)
featureImportance <- data.frame(Feature=row.names(imp), Importance=imp[,1])

ggplot(featureImportance, aes(x=reorder(Feature, Importance), y=Importance)) +
    geom_bar(stat="identity", fill="#53cfff") +
    coord_flip() + 
    theme_light(base_size=20) +
    xlab("") +
    ylab("Importance") + 
    ggtitle("Random Forest Feature Importance\n") +
    theme(plot.title=element_text(size=18))
```

### *The accuracy using random forest is `0.8215438`*

establish the output for testset

```{r}
testSet$Pclass <- factor(testSet$Pclass)
testSet$totalAge <- ifelse(is.na(testSet$Age), 
                       mean(testSet$Age, na.rm = TRUE), 
                       testSet$Age)
testSet$Survived <- predict(model, newdata = testSet)
submission <- testSet[,c("PassengerId", "Survived")]
write.table(submission, file = "submission.csv", 
            col.names = TRUE, row.names = FALSE, sep = ",")
```

### Now we can change the algorithms and the combination of variables to get some new results:

```{r}
# first of all, because we need to use this process several
# times, we will build a function to save the repetitive work
train_the_data <- function(method_use, isUsingEmbarked) {
    # set the seed first so you can get the same result as me
    set.seed(as.numeric(as.Date("2017-05-15")))
    # method_use argument means using method in train function
    # if isUsingEmbarked argument is TRUE, the variables will be 
    # Pclass + totalAge + Sex + SibSp + Embarked, otherwise will 
    # be Pclass + totalAge + Sex + SibSp
    if (isUsingEmbarked == TRUE) {
        model <- train(Survived ~ Pclass + totalAge + Sex + SibSp + Embarked, 
                       data = trainSet, 
                       # Use the trainSet dataframe as the training data
                       method = method_use,
                       # Use the algorithm we defined
                       trControl = trainControl(method = "cv", 
                                                # Use cross-validation
                                                number = 10) 
                       # Use 10 folds for cross-validation
        )    
    }
    else {
        model <- train(Survived ~ Pclass + totalAge + Sex + SibSp, 
                       data = trainSet, 
                       # Use the trainSet dataframe as the training data
                       method = method_use,
                       # Use the algorithm we defined
                       trControl = trainControl(method = "cv", 
                                                # Use cross-validation
                                                number = 10) 
                       # Use 10 folds for cross-validation
        ) 
    }
    return(model)    
}
```

### Neural Network

```{r}
# test with Embarked
model <- train_the_data("nnet", TRUE)
model
# the accuracy rate is 0.8170877
# test with no Embarked
model <- train_the_data("nnet", FALSE)
model
# the accuracy rate is 0.8147514
```

### SVM

```{r}
# test with Embarked
model <- train_the_data("svmLinearWeights2", TRUE)
model
# the accuracy is 0.7911931
# test with no Embarked
model <- train_the_data("svmLinearWeights2", FALSE)
model
# the accuracy is 0.7923417
```

### Desicion Tree

```{r}
# test with Embarked
model <- train_the_data("rpart", TRUE)
model
# the accuracy is 0.8114184
# test with no Embarked
model <- train_the_data("rpart", FALSE)
model
# the accuracy is 0.7990333
```

### knn

```{r}
# test with Embarked
model <- train_the_data("knn", TRUE)
model
# the accuracy is 0.7486715
# test with no Embarked
model <- train_the_data("knn", FALSE)
model
# the accuracy is 0.7621555
```

### Naive Bayes

```{r}
# test with Embarked
model <- train_the_data("nb", TRUE)
model
# the accuracy is 0.7631151
# test with no Embarked
model <- train_the_data("nb", FALSE)
model
# the accuracy is 0.7889576
```

============================

##                            Conclusion

*the accuracy rate `0.8215438` is the best rate, using random forest with cv algorithm to do the prediction*

============================